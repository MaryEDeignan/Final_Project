{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment and run if not already installed)\n",
    "# !pip install torch torch-geometric transformers pandas scikit-learn networkx\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Transformer models\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Torch Geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class RecipeDirectionClassifier:\n",
    "    def __init__(self, dataframe, test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the classifier for recipe direction classification\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataframe : pandas.DataFrame\n",
    "            DataFrame with 'text' and 'classification' columns\n",
    "        test_size : float, optional (default=0.2)\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, optional (default=42)\n",
    "            Controls the shuffling applied to the data before splitting\n",
    "        \"\"\"\n",
    "        # Ensure classification column is numeric\n",
    "        dataframe['classification'] = dataframe['classification'].astype(float)\n",
    "        \n",
    "        # Split data\n",
    "        self.train_texts, self.test_texts, self.train_labels, self.test_labels = train_test_split(\n",
    "            dataframe['text'].values, \n",
    "            dataframe['classification'].values, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Load pre-trained model for embeddings\n",
    "        print(\"Loading BERT model and tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.embedding_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Freeze embedding model parameters\n",
    "        for param in self.embedding_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def get_text_embeddings(self, texts):\n",
    "        \"\"\"\n",
    "        Generate embeddings for texts using BERT\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        texts : list or numpy array\n",
    "            List of text documents\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            Embedded representations of texts\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for text in texts:\n",
    "            # Tokenize and get embeddings\n",
    "            inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512, padding=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.embedding_model(**inputs)\n",
    "                # Use mean pooling for text embedding\n",
    "                embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "            \n",
    "            embeddings.append(embedding)\n",
    "        \n",
    "        # Convert list to numpy array for efficient tensor creation\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def create_text_graph(self, texts, labels):\n",
    "        \"\"\"\n",
    "        Create a graph representation of text data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        texts : list or numpy array\n",
    "            List of text documents\n",
    "        labels : list or numpy array\n",
    "            Corresponding labels for texts\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        torch_geometric.data.Data\n",
    "            Graph data object for training\n",
    "        \"\"\"\n",
    "        # Get text embeddings\n",
    "        embeddings = self.get_text_embeddings(texts)\n",
    "        \n",
    "        # Create graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes with embeddings and labels\n",
    "        for i in range(len(embeddings)):\n",
    "            G.add_node(i, embedding=embeddings[i], label=labels[i])\n",
    "        \n",
    "        # Connect nodes based on embedding similarity\n",
    "        for i in range(len(embeddings)):\n",
    "            for j in range(i+1, len(embeddings)):\n",
    "                similarity = self.cosine_similarity(embeddings[i], embeddings[j])\n",
    "                if similarity > 0.7:  # Adjust threshold as needed\n",
    "                    G.add_edge(i, j, weight=similarity)\n",
    "        \n",
    "        # Prepare tensors\n",
    "        if len(G.nodes) == 0:\n",
    "            # Fallback if no graph is created\n",
    "            x = torch.tensor(embeddings, dtype=torch.float32)\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            y = torch.tensor(labels, dtype=torch.float32)\n",
    "        else:\n",
    "            # Efficiently create node embeddings tensor\n",
    "            node_embeddings = np.array([G.nodes[n]['embedding'] for n in G.nodes])\n",
    "            x = torch.tensor(node_embeddings, dtype=torch.float32)\n",
    "            \n",
    "            # Convert edges to tensor\n",
    "            edge_index = torch.tensor(list(G.edges)).t().contiguous() if G.edges else torch.empty((2, 0), dtype=torch.long)\n",
    "            \n",
    "            # Create labels tensor\n",
    "            y = torch.tensor([G.nodes[n]['label'] for n in G.nodes], dtype=torch.float32)\n",
    "        \n",
    "        return Data(x=x, edge_index=edge_index, y=y)\n",
    "    \n",
    "    def cosine_similarity(self, vec1, vec2):\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two vectors\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        vec1, vec2 : numpy.ndarray\n",
    "            Input vectors\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Cosine similarity between vectors\n",
    "        \"\"\"\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "class GraphTextClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        Graph Convolutional Network for binary classification\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Dimension of input features\n",
    "        hidden_dim : int\n",
    "            Dimension of hidden layer\n",
    "        \"\"\"\n",
    "        super(GraphTextClassifier, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, 1)  # Binary classification\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass through the graph neural network\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Node features\n",
    "        edge_index : torch.Tensor\n",
    "            Graph connectivity\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Predicted binary class probabilities\n",
    "        \"\"\"\n",
    "        # Handle case where edge_index might be empty\n",
    "        if edge_index.numel() == 0:\n",
    "            # If no edges, use simple linear transformation\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = self.conv2(x, edge_index)\n",
    "        else:\n",
    "            x = self.conv1(x, edge_index).relu()\n",
    "            x = F.dropout(x, training=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "def train_graph_classifier(classifier, verbose=True):\n",
    "    \"\"\"\n",
    "    Train the graph neural network classifier\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    classifier : RecipeDirectionClassifier\n",
    "        Prepared classifier for training\n",
    "    verbose : bool, optional\n",
    "        Whether to print training progress\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Trained model and training metrics\n",
    "    \"\"\"\n",
    "    # Prepare graph data\n",
    "    train_graph = classifier.create_text_graph(classifier.train_texts, classifier.train_labels)\n",
    "    test_graph = classifier.create_text_graph(classifier.test_texts, classifier.test_labels)\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    input_dim = train_graph.x.shape[1]\n",
    "    hidden_dim = 64\n",
    "    \n",
    "    # Initialize model\n",
    "    model = GraphTextClassifier(input_dim, hidden_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    best_accuracy = 0\n",
    "    training_history = {'loss': [], 'accuracy': []}\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Handle potential shape differences\n",
    "        out = model(train_graph.x, train_graph.edge_index).squeeze()\n",
    "        \n",
    "        # Ensure out and train_graph.y have compatible shapes\n",
    "        if out.shape != train_graph.y.shape:\n",
    "            out = out[:train_graph.y.shape[0]]\n",
    "        \n",
    "        loss = F.binary_cross_entropy(out, train_graph.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_out = model(test_graph.x, test_graph.edge_index).squeeze()\n",
    "            \n",
    "            # Ensure test_out and test_graph.y have compatible shapes\n",
    "            if test_out.shape != test_graph.y.shape:\n",
    "                test_out = test_out[:test_graph.y.shape[0]]\n",
    "            \n",
    "            test_pred = (test_out > 0.5).float()\n",
    "            accuracy = (test_pred == test_graph.y).float().mean()\n",
    "            \n",
    "            # Track training history\n",
    "            training_history['loss'].append(loss.item())\n",
    "            training_history['accuracy'].append(accuracy.item())\n",
    "            \n",
    "            # Track best model\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model.state_dict().copy()\n",
    "        \n",
    "        # Verbose output\n",
    "        if verbose and (epoch % 10 == 0):\n",
    "            print(f\"Epoch {epoch}: Loss = {loss.item():.4f}, Accuracy = {accuracy.item():.4f}\")\n",
    "    \n",
    "    # Restore best model\n",
    "    model.load_state_dict(best_model)\n",
    "    \n",
    "    return model, {\n",
    "        'train_loss': loss.item(),\n",
    "        'test_accuracy': best_accuracy.item(),\n",
    "        'training_history': training_history\n",
    "    }\n",
    "\n",
    "# Visualization function (optional, requires matplotlib)\n",
    "def plot_training_history(metrics):\n",
    "    \"\"\"\n",
    "    Plot training loss and accuracy\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    metrics : dict\n",
    "        Training metrics dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    history = metrics['training_history']\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['loss'])\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.title('Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage in notebook\n",
    "def main(df):\n",
    "    # Sample DataFrame with recipe directions\n",
    "    # Print initial dataset\n",
    "    print(\"Dataset Overview:\")\n",
    "    print(df)\n",
    "    print(\"\\nDataset Shape:\", df.shape)\n",
    "    \n",
    "    # Create classifier\n",
    "    recipe_classifier = RecipeDirectionClassifier(df)\n",
    "    \n",
    "    # Train classifier\n",
    "    model, metrics = train_graph_classifier(recipe_classifier)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nTraining Metrics:\")\n",
    "    print(f\"Train Loss: {metrics['train_loss']:.4f}\")\n",
    "    print(f\"Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "    \n",
    "    # Optional: Plot training history\n",
    "    try:\n",
    "        plot_training_history(metrics)\n",
    "    except ImportError:\n",
    "        print(\"Matplotlib not available for plotting\")\n",
    "    \n",
    "    return model, metrics\n",
    "\n",
    "# If running in Jupyter, you can simply call main()\n",
    "# model, metrics = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/both_likes_dislikes.csv')\n",
    "data = data[['directions', 'like_or_dislike']]\n",
    "\n",
    "data = data.rename(columns={'directions':'text', 'like_or_dislike':'classification'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "                                                text  classification\n",
      "0  ['Heat a large skillet over medium-high heat. ...               1\n",
      "1  ['Preheat the oven to 350 degrees F (175 degre...               0\n",
      "2  ['Preheat the oven to 350 degrees F (175 degre...               1\n",
      "3  ['Place a paper towel on a microwave-safe plat...               0\n",
      "4  ['Preheat the oven to 350 degrees F (175 degre...               1\n",
      "\n",
      "Dataset Shape: (5, 2)\n",
      "Loading BERT model and tokenizer...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "slice() cannot be applied to a 0-dim tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 323\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    320\u001b[0m recipe_classifier \u001b[38;5;241m=\u001b[39m RecipeDirectionClassifier(df)\n\u001b[0;32m    322\u001b[0m \u001b[38;5;66;03m# Train classifier\u001b[39;00m\n\u001b[1;32m--> 323\u001b[0m model, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_graph_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecipe_classifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# Print metrics\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 254\u001b[0m, in \u001b[0;36mtrain_graph_classifier\u001b[1;34m(classifier, verbose)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# Ensure test_out and test_graph.y have compatible shapes\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_out\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m test_graph\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m--> 254\u001b[0m     test_out \u001b[38;5;241m=\u001b[39m \u001b[43mtest_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mtest_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    256\u001b[0m test_pred \u001b[38;5;241m=\u001b[39m (test_out \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    257\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m (test_pred \u001b[38;5;241m==\u001b[39m test_graph\u001b[38;5;241m.\u001b[39my)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[1;31mIndexError\u001b[0m: slice() cannot be applied to a 0-dim tensor."
     ]
    }
   ],
   "source": [
    "model, metrics = main(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# [Previous code remains the same as in the original script]\n",
    "\n",
    "class RecipeDirectionClassifier:\n",
    "    def __init__(self, dataframe, test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the classifier for recipe direction classification\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataframe : pandas.DataFrame\n",
    "            DataFrame with 'text' and 'classification' columns\n",
    "        test_size : float, optional (default=0.2)\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, optional (default=42)\n",
    "            Controls the shuffling applied to the data before splitting\n",
    "        \"\"\"\n",
    "        # Ensure classification column is numeric\n",
    "        dataframe['classification'] = dataframe['classification'].astype(float)\n",
    "        \n",
    "        # Split data\n",
    "        self.train_texts, self.test_texts, self.train_labels, self.test_labels = train_test_split(\n",
    "            dataframe['text'].values, \n",
    "            dataframe['classification'].values, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Load pre-trained model for embeddings\n",
    "        print(\"Loading BERT model and tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.embedding_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Freeze embedding model parameters\n",
    "        for param in self.embedding_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Store the trained graph neural network model\n",
    "        self.graph_model = None\n",
    "    \n",
    "def get_text_embeddings(self, texts):\n",
    "        \"\"\"\n",
    "        Generate embeddings for texts using BERT\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        texts : list or numpy array\n",
    "            List of text documents\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            Embedded representations of texts\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for text in texts:\n",
    "            # Tokenize and get embeddings\n",
    "            inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512, padding=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.embedding_model(**inputs)\n",
    "                # Use mean pooling for text embedding\n",
    "                embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "            \n",
    "            embeddings.append(embedding)\n",
    "        \n",
    "        # Convert list to numpy array for efficient tensor creation\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def create_text_graph(self, texts, labels):\n",
    "        \"\"\"\n",
    "        Create a graph representation of text data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        texts : list or numpy array\n",
    "            List of text documents\n",
    "        labels : list or numpy array\n",
    "            Corresponding labels for texts\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        torch_geometric.data.Data\n",
    "            Graph data object for training\n",
    "        \"\"\"\n",
    "        # Get text embeddings\n",
    "        embeddings = self.get_text_embeddings(texts)\n",
    "        \n",
    "        # Create graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes with embeddings and labels\n",
    "        for i in range(len(embeddings)):\n",
    "            G.add_node(i, embedding=embeddings[i], label=labels[i])\n",
    "        \n",
    "        # Connect nodes based on embedding similarity\n",
    "        for i in range(len(embeddings)):\n",
    "            for j in range(i+1, len(embeddings)):\n",
    "                similarity = self.cosine_similarity(embeddings[i], embeddings[j])\n",
    "                if similarity > 0.7:  # Adjust threshold as needed\n",
    "                    G.add_edge(i, j, weight=similarity)\n",
    "        \n",
    "        # Prepare tensors\n",
    "        if len(G.nodes) == 0:\n",
    "            # Fallback if no graph is created\n",
    "            x = torch.tensor(embeddings, dtype=torch.float32)\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            y = torch.tensor(labels, dtype=torch.float32)\n",
    "        else:\n",
    "            # Efficiently create node embeddings tensor\n",
    "            node_embeddings = np.array([G.nodes[n]['embedding'] for n in G.nodes])\n",
    "            x = torch.tensor(node_embeddings, dtype=torch.float32)\n",
    "            \n",
    "            # Convert edges to tensor\n",
    "            edge_index = torch.tensor(list(G.edges)).t().contiguous() if G.edges else torch.empty((2, 0), dtype=torch.long)\n",
    "            \n",
    "            # Create labels tensor\n",
    "            y = torch.tensor([G.nodes[n]['label'] for n in G.nodes], dtype=torch.float32)\n",
    "        \n",
    "        return Data(x=x, edge_index=edge_index, y=y)\n",
    "    \n",
    "    def cosine_similarity(self, vec1, vec2):\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two vectors\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        vec1, vec2 : numpy.ndarray\n",
    "            Input vectors\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Cosine similarity between vectors\n",
    "        \"\"\"\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "class GraphTextClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        Graph Convolutional Network for binary classification\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Dimension of input features\n",
    "        hidden_dim : int\n",
    "            Dimension of hidden layer\n",
    "        \"\"\"\n",
    "        super(GraphTextClassifier, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, 1)  # Binary classification\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass through the graph neural network\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Node features\n",
    "        edge_index : torch.Tensor\n",
    "            Graph connectivity\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Predicted binary class probabilities\n",
    "        \"\"\"\n",
    "        # Handle case where edge_index might be empty\n",
    "        if edge_index.numel() == 0:\n",
    "            # If no edges, use simple linear transformation\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = self.conv2(x, edge_index)\n",
    "        else:\n",
    "            x = self.conv1(x, edge_index).relu()\n",
    "            x = F.dropout(x, training=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "def train_graph_classifier(classifier, verbose=True):\n",
    "    \"\"\"\n",
    "    Train the graph neural network classifier\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    classifier : RecipeDirectionClassifier\n",
    "        Prepared classifier for training\n",
    "    verbose : bool, optional\n",
    "        Whether to print training progress\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Trained model and training metrics\n",
    "    \"\"\"\n",
    "    # Prepare graph data\n",
    "    train_graph = classifier.create_text_graph(classifier.train_texts, classifier.train_labels)\n",
    "    test_graph = classifier.create_text_graph(classifier.test_texts, classifier.test_labels)\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    input_dim = train_graph.x.shape[1]\n",
    "    hidden_dim = 64\n",
    "    \n",
    "    # Initialize model\n",
    "    model = GraphTextClassifier(input_dim, hidden_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    best_accuracy = 0\n",
    "    training_history = {'loss': [], 'accuracy': []}\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Handle potential shape differences\n",
    "        out = model(train_graph.x, train_graph.edge_index).squeeze()\n",
    "        \n",
    "        # Ensure out and train_graph.y have compatible shapes\n",
    "        if out.shape != train_graph.y.shape:\n",
    "            out = out[:train_graph.y.shape[0]]\n",
    "        \n",
    "        loss = F.binary_cross_entropy(out, train_graph.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_out = model(test_graph.x, test_graph.edge_index).squeeze()\n",
    "            \n",
    "            # Ensure test_out and test_graph.y have compatible shapes\n",
    "            if test_out.shape != test_graph.y.shape:\n",
    "                test_out = test_out[:test_graph.y.shape[0]]\n",
    "            \n",
    "            test_pred = (test_out > 0.5).float()\n",
    "            accuracy = (test_pred == test_graph.y).float().mean()\n",
    "            \n",
    "            # Track training history\n",
    "            training_history['loss'].append(loss.item())\n",
    "            training_history['accuracy'].append(accuracy.item())\n",
    "            \n",
    "            # Track best model\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model.state_dict().copy()\n",
    "        \n",
    "        # Verbose output\n",
    "        if verbose and (epoch % 10 == 0):\n",
    "            print(f\"Epoch {epoch}: Loss = {loss.item():.4f}, Accuracy = {accuracy.item():.4f}\")\n",
    "    \n",
    "    # Restore best model\n",
    "    model.load_state_dict(best_model)\n",
    "    \n",
    "    return model, {\n",
    "        'train_loss': loss.item(),\n",
    "        'test_accuracy': best_accuracy.item(),\n",
    "        'training_history': training_history\n",
    "    }\n",
    "\n",
    "# Visualization function (optional, requires matplotlib)\n",
    "def plot_training_history(metrics):\n",
    "    \"\"\"\n",
    "    Plot training loss and accuracy\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    metrics : dict\n",
    "        Training metrics dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    history = metrics['training_history']\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['loss'])\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.title('Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# If running in Jupyter, you can simply call main()\n",
    "# model, metrics = main()\n",
    "    \n",
    "def predict(self, texts, trained_model=None):\n",
    "    \"\"\"\n",
    "    Predict classifications for new text samples\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    texts : list or numpy array\n",
    "        List of text documents to classify\n",
    "    trained_model : torch.nn.Module, optional\n",
    "        Trained graph neural network model (if not provided, uses self.graph_model)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Predicted probabilities for each text sample\n",
    "    \"\"\"\n",
    "    if trained_model is None:\n",
    "        if self.graph_model is None:\n",
    "            raise ValueError(\"No trained model available. Train the model first.\")\n",
    "        trained_model = self.graph_model\n",
    "        \n",
    "    # Ensure the model is in evaluation mode\n",
    "    trained_model.eval()\n",
    "        \n",
    "    # Create a graph from the input texts\n",
    "    predict_graph = self.create_text_graph(texts, np.zeros(len(texts)))\n",
    "        \n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        # Get predictions\n",
    "        predictions = trained_model(predict_graph.x, predict_graph.edge_index).numpy()\n",
    "    \n",
    "    # Return probabilities\n",
    "    return predictions.flatten()\n",
    "    \n",
    "def train_and_save_model(self, verbose=True):\n",
    "    \"\"\"\n",
    "    Train the graph classifier and save the model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    verbose : bool, optional\n",
    "        Whether to print training progress\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Training metrics\n",
    "    \"\"\"\n",
    "    # Train the model\n",
    "    self.graph_model, metrics = train_graph_classifier(self, verbose)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Modify main function to demonstrate prediction\n",
    "def main(df):\n",
    "    # Sample DataFrame with recipe directions\n",
    "    print(\"Dataset Overview:\")\n",
    "    print(df)\n",
    "    print(\"\\nDataset Shape:\", df.shape)\n",
    "    \n",
    "    # Create classifier\n",
    "    recipe_classifier = RecipeDirectionClassifier(df)\n",
    "    \n",
    "    # Train and save the model\n",
    "    metrics = recipe_classifier.train_and_save_model()\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nTraining Metrics:\")\n",
    "    print(f\"Train Loss: {metrics['train_loss']:.4f}\")\n",
    "    print(f\"Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "    \n",
    "    # Example prediction\n",
    "    example_texts = [\n",
    "        \"Preheat the oven and mix ingredients carefully\",\n",
    "        \"Chop vegetables into small, uniform pieces\",\n",
    "        \"Stir continuously to prevent burning\"\n",
    "    ]\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = recipe_classifier.predict(example_texts)\n",
    "    \n",
    "    print(\"\\nPrediction Examples:\")\n",
    "    for text, pred in zip(example_texts, predictions):\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Probability of Class 1: {pred:.4f}\")\n",
    "        print(f\"Predicted Class: {'1' if pred > 0.5 else '0'}\\n\")\n",
    "    \n",
    "    # Optional: Plot training history\n",
    "    try:\n",
    "        plot_training_history(metrics)\n",
    "    except ImportError:\n",
    "        print(\"Matplotlib not available for plotting\")\n",
    "    \n",
    "    return recipe_classifier, metrics\n",
    "\n",
    "# Note: In a Jupyter notebook, you would call this with your DataFrame\n",
    "# classifier, metrics = main(your_dataframe)\n",
    "\n",
    "# Add a convenience method for easy prediction threshold adjustment\n",
    "def adjust_prediction_threshold(predictions, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Adjust predictions based on a custom threshold\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : numpy.ndarray\n",
    "        Predicted probabilities\n",
    "    threshold : float, optional (default=0.5)\n",
    "        Threshold for binary classification\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Binary predictions\n",
    "    \"\"\"\n",
    "    return (predictions > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "                                                text  classification\n",
      "0  ['Heat a large skillet over medium-high heat. ...             1.0\n",
      "1  ['Preheat the oven to 350 degrees F (175 degre...             0.0\n",
      "2  ['Preheat the oven to 350 degrees F (175 degre...             1.0\n",
      "3  ['Place a paper towel on a microwave-safe plat...             0.0\n",
      "4  ['Preheat the oven to 350 degrees F (175 degre...             1.0\n",
      "\n",
      "Dataset Shape: (5, 2)\n",
      "Loading BERT model and tokenizer...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RecipeDirectionClassifier' object has no attribute 'train_and_save_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 373\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    370\u001b[0m recipe_classifier \u001b[38;5;241m=\u001b[39m RecipeDirectionClassifier(df)\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# Train and save the model\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mrecipe_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_save_model\u001b[49m()\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# Print metrics\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RecipeDirectionClassifier' object has no attribute 'train_and_save_model'"
     ]
    }
   ],
   "source": [
    "main(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# [Previous code remains the same as in the original script]\n",
    "\n",
    "def visualize_text_network(classifier, graph_data, title='Text Similarity Network'):\n",
    "    \"\"\"\n",
    "    Visualize the text similarity network\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    classifier : RecipeDirectionClassifier\n",
    "        The classifier used to create the graph\n",
    "    graph_data : torch_geometric.data.Data\n",
    "        Graph data object\n",
    "    title : str, optional\n",
    "        Title of the visualization\n",
    "    \"\"\"\n",
    "    # Convert graph data to NetworkX\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes with their embeddings and labels\n",
    "    for i in range(graph_data.x.shape[0]):\n",
    "        node_embedding = graph_data.x[i].numpy()\n",
    "        node_label = graph_data.y[i].item()\n",
    "        G.add_node(i, embedding=node_embedding, label=node_label)\n",
    "    \n",
    "    # Add edges\n",
    "    if graph_data.edge_index.numel() > 0:\n",
    "        edges = graph_data.edge_index.t().numpy()\n",
    "        for edge in edges:\n",
    "            G.add_edge(edge[0], edge[1])\n",
    "    \n",
    "    # Prepare visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Use spring layout for node positioning\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "    \n",
    "    # Color nodes based on their label\n",
    "    node_colors = [('red' if G.nodes[node]['label'] == 1 else 'blue') for node in G.nodes()]\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=100, alpha=0.7)\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.3)\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Create a custom legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='red', marker='o', linestyle='None', \n",
    "               markersize=10, label='Class 1'),\n",
    "        Line2D([0], [0], color='blue', marker='o', linestyle='None', \n",
    "               markersize=10, label='Class 0')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Modify main function to include network visualization\n",
    "def main(df):\n",
    "    # Sample DataFrame with recipe directions\n",
    "    # Print initial dataset\n",
    "    print(\"Dataset Overview:\")\n",
    "    print(df)\n",
    "    print(\"\\nDataset Shape:\", df.shape)\n",
    "    \n",
    "    # Create classifier\n",
    "    recipe_classifier = RecipeDirectionClassifier(df)\n",
    "    \n",
    "    # Create graph data\n",
    "    #train_graph = recipe_classifier.create_text_graph(recipe_classifier.train_texts, recipe_classifier.train_labels)\n",
    "    \n",
    "    # Visualize the network\n",
    "    #visualize_text_network(recipe_classifier, train_graph, title='Recipe Direction Similarity Network')\n",
    "    \n",
    "    # Train classifier\n",
    "    model, metrics = train_graph_classifier(recipe_classifier)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nTraining Metrics:\")\n",
    "    print(f\"Train Loss: {metrics['train_loss']:.4f}\")\n",
    "    print(f\"Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "    \n",
    "    # Optional: Plot training history\n",
    "    try:\n",
    "        plot_training_history(metrics)\n",
    "    except ImportError:\n",
    "        print(\"Matplotlib not available for plotting\")\n",
    "    \n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "                                                 text  classification\n",
      "0   ['Heat a large skillet over medium-high heat. ...             1.0\n",
      "1   ['Preheat the oven to 350 degrees F (175 degre...             0.0\n",
      "2   ['Preheat the oven to 350 degrees F (175 degre...             1.0\n",
      "3   ['Place a paper towel on a microwave-safe plat...             1.0\n",
      "4   ['Preheat the oven to 350 degrees F (175 degre...             1.0\n",
      "5   ['Preheat oven to 400 degrees F (200 degrees C...             1.0\n",
      "6   ['Dissolve salt in warm water, and mix in 1 cu...             0.0\n",
      "7   ['Heat 2 tablespoons of olive oil in a large p...             1.0\n",
      "8   ['Preheat oven to 375 degrees F (190 degrees C...             1.0\n",
      "9   [\"Set oven rack about 6 inches from the heat s...             1.0\n",
      "10  ['Preheat oven to 400 degrees F (200 degrees C...             1.0\n",
      "11  ['Mix chili powder, cumin, paprika, oregano, p...             1.0\n",
      "\n",
      "Dataset Shape: (12, 2)\n",
      "Loading BERT model and tokenizer...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RecipeDirectionClassifier' object has no attribute 'create_text_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[35], line 92\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     83\u001b[0m recipe_classifier \u001b[38;5;241m=\u001b[39m RecipeDirectionClassifier(df)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Create graph data\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m#train_graph = recipe_classifier.create_text_graph(recipe_classifier.train_texts, recipe_classifier.train_labels)\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Train classifier\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m model, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_graph_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecipe_classifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Print metrics\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[31], line 217\u001b[0m, in \u001b[0;36mtrain_graph_classifier\u001b[1;34m(classifier, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03mTrain the graph neural network classifier\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m    Trained model and training metrics\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# Prepare graph data\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m train_graph \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_text_graph\u001b[49m(classifier\u001b[38;5;241m.\u001b[39mtrain_texts, classifier\u001b[38;5;241m.\u001b[39mtrain_labels)\n\u001b[0;32m    218\u001b[0m test_graph \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mcreate_text_graph(classifier\u001b[38;5;241m.\u001b[39mtest_texts, classifier\u001b[38;5;241m.\u001b[39mtest_labels)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# Model hyperparameters\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RecipeDirectionClassifier' object has no attribute 'create_text_graph'"
     ]
    }
   ],
   "source": [
    "main(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, classifier = main(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
